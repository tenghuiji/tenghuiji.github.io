{"pages":[{"title":"VM_Ware_EXSI创建虚拟机","text":"VM Ware EXSI创建虚拟机 第一步创建虚拟机 点击图片的 Create/Register VM 选择新建一个虚拟机 In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"VM","url":"VM Ware EXSI.html"},{"title":"天眼查爬虫","text":"天眼查爬虫 2018-1-21 marking的同事跑过来求帮忙，能不能通过爬虫帮他，根据会议收集表格中，人员名单所填写的公司名称添加一列行业信息。 他目前是手工到天眼输入公司名称，然后再取出觉得匹配的公司，最后在找到行业分类。但是人员明天太多了，做这么枯燥的工作他要疯了:( 我觉得这个应该比较简单，答应他帮他写个爬虫 确定爬取方式 selenium是个自动化测试工具，他可以通过浏览器来模拟各种操作实现测试页面功能 BeautifulSoup是个爬虫常用的Dom解析工具 lxml提供了xpath获取方法，强烈推荐通过xpath访问节点，配合Chrome的inspect工具中的 copy xpath可以节省大量的节点获取工作量 安装必要的工具 pip install selenium pip install lxml pip install beautifulsoup4 下载最新的chromedriver http://chromedriver.storage.googleapis.com/index.html 我是mac 安装到/usr/local/bin/目录下，然后运行 chromedriver -v查看是否安装好了 常见问题 公司名称不存在；在天眼搜里面找不到公司名称就在程序中填写none或者其他的字符说明 相似公司很多个不确定怎么选；这个比较不好处理，现在是取天眼搜中查找到的top 1 记录 我想人选择的话可能会根据与会者所在地配合来查找吧，例如是在北京参加的，那么就有限找北京的同名（名称相似）的公司 天眼里面找到的母公司下面有多个子公司，导致鼠标点击不到连接，必须拉滚动条才能解决问题 In [30]: from selenium import webdriver from selenium.common.exceptions import TimeoutException from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import pandas as pd from bs4 import BeautifulSoup from lxml import etree import time import random In [31]: def search ( keywords ): # print('正在搜索:{}'.format(keywords)) try : browser . get ( 'https://www.tianyancha.com/' ) # 找到输入框 input = wait . until ( EC . presence_of_element_located ( ( By . CSS_SELECTOR , '.main-tab-outer .main-tab-body .inputV2 input' ))) # input.send_keys(keywords.decode('utf-8'))#中英混输入可防止乱码 input . send_keys ( keywords ) time . sleep ( random . sample ([ 1 , 2 , 3 , 4 , 5 ], 1 )[ 0 ]) submit = wait . until ( EC . element_to_be_clickable ( ( By . CSS_SELECTOR , '.mainv2_tab1 .inputV2 .search_button.white-btn' ))) submit . click () time . sleep ( random . sample ([ 1 , 2 , 3 , 4 , 5 ], 1 )[ 0 ]) # 获取打开的多个窗口句柄 windows = browser . window_handles time . sleep ( random . sample ([ 1 , 2 , 3 , 4 , 5 ], 1 )[ 0 ]) # 切换到当前最新打开的窗口 browser . switch_to . window ( windows [ - 1 ]) time . sleep ( random . sample ([ 1 , 2 , 3 , 4 , 5 ], 1 )[ 0 ]) # 用目标元素参考去拖动 target_elem = browser . find_element_by_xpath ( '//*[@id=\"web-content\"]/div/div/div/div[1]/div[3]/div[1]/div[1]' ) js = 'arguments[0].scrollIntoView();' browser . execute_script ( js , target_elem ) # 在搜索结果中获取第一个公司 top1 = wait . until ( EC . element_to_be_clickable (( By . CSS_SELECTOR , '.search_result_container a' ))) top1 . click () # 关闭当前窗口 browser . close () # 获取打开的多个窗口句柄 windows = browser . window_handles time . sleep ( random . sample ([ 1 , 2 , 3 , 4 , 5 ], 1 )[ 0 ]) # 切换到当前最新打开的窗口 browser . switch_to . window ( windows [ - 1 ]) time . sleep ( random . sample ([ 1 , 2 , 3 , 4 , 5 ], 1 )[ 0 ]) # 等待页面全部加载完成 wait . until ( EC . presence_of_all_elements_located (( By . CSS_SELECTOR , '#_container_baseInfo' ))) selector = etree . HTML ( browser . page_source ) # 获取公司名称 company_name = selector . xpath ( '//*[@id=\"company_web_top\"]/div[2]/div[2]/div[1]/span/text()' ) # 获取公司信息表格中的行业信息xpath //*[@id=\"_container_baseInfo\"]/div/div[3]/table/tbody/tr[3]/td[4]/text() links = selector . xpath ( '//*[@id=\"_container_baseInfo\"]/div/div[3]/table/tbody/tr[3]/td[4]/text()' ) time . sleep ( random . sample ([ 1 , 2 , 3 , 4 , 5 ], 1 )[ 0 ]) for link in links : s = '公司名称: {} ,找到的公司: {} ,所属行业: {} ' . format ( keywords , company_name [ 0 ], link ) print ( s ) return s # browser.close() except Exception : s = '公司名称: {} ,找到的公司: {} ,所属行业: {} ' . format ( keywords , 'NaN' , 'NaN' ) print ( '公司名称: {} ,找到的公司: {} ,所属行业: {} ' . format ( keywords , 'NaN' , 'NaN' )) return s #pass In [36]: #跟新数据 def updateString ( ss ): return ss [ ss . find ( ',' ) + 1 :] #提取查找输入的公司名称 def splitCompanyNameColumn ( ss ): print ( ss ) return ss [ ss . find ( ':' ) + 1 : ss . find ( ',' )] #提取找到的公司名称 def splitFindCompanyNameColumn ( ss ): print ( ss ) return ss [ ss . find ( ':' ) + 1 : ss . find ( ',' )] #提取找到的所属行业 def splitIndustryColumn ( ss ): print ( ss ) return ss [ ss . find ( ':' ) + 1 : ss . find ( ',' )] In [39]: # 启动浏览器，executable_path路径要根据自己chromedriver.exe的位置更改 browser = webdriver . Chrome ( executable_path = r '/usr/local/bin/chromedriver' ) # 设置浏览器窗口位置及大小 browser . set_window_rect ( x = 0 , y = 0 , width = 1200 , height = 750 ) # 设定页面加载限制时间 browser . set_page_load_timeout ( 30 ) # 设置锁定标签等待时长 wait = WebDriverWait ( browser , 20 ) df = pd . read_csv ( './dataset/Jam_data_analysis.csv' ) df [ '临时' ] = df [ '公司' ] . apply ( search ) df [ '公司名称' ] = df [ '临时' ] . apply ( splitCompanyNameColumn ) df [ '临时' ] = df [ '临时' ] . apply ( updateString ) df [ '找到的公司' ] = df [ '临时' ] . apply ( splitFindCompanyNameColumn ) df [ '临时' ] = df [ '临时' ] . apply ( updateString ) df [ '所属行业' ] = df [ '临时' ] . apply ( splitIndustryColumn ) #删除临时字段 del df [ '临时' ] #关闭浏览器 browser . close () #保存结果到csv文件中 df . to_csv ( './dataset/output.csv' ) 公司名称:小米,找到的公司:小米科技有限责任公司,所属行业:科技推广和应用服务业 公司名称:猎豹汽车研究院,找到的公司:NaN,所属行业:NaN 公司名称:滴滴出行,找到的公司:NaN,所属行业:NaN 0 公司名称:小米,找到的公司:小米科技有限责任公司,所属行业:科技推广和应用服务业 1 公司名称:猎豹汽车研究院,找到的公司:NaN,所属行业:NaN 2 公司名称:滴滴出行,找到的公司:NaN,所属行业:NaN Name: 临时, dtype: object 公司名称:小米,找到的公司:小米科技有限责任公司,所属行业:科技推广和应用服务业 公司名称:猎豹汽车研究院,找到的公司:NaN,所属行业:NaN 公司名称:滴滴出行,找到的公司:NaN,所属行业:NaN 0 找到的公司:小米科技有限责任公司,所属行业:科技推广和应用服务业 1 找到的公司:NaN,所属行业:NaN 2 找到的公司:NaN,所属行业:NaN Name: 临时, dtype: object 找到的公司:小米科技有限责任公司,所属行业:科技推广和应用服务业 找到的公司:NaN,所属行业:NaN 找到的公司:NaN,所属行业:NaN 0 所属行业:科技推广和应用服务业 1 所属行业:NaN 2 所属行业:NaN Name: 临时, dtype: object 所属行业:科技推广和应用服务业 所属行业:NaN 所属行业:NaN if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"spider","url":"spider-tianyancha.html"},{"title":"北京 PM2.5 相关性分析","text":"北京PM2.5相关性分析 In [2]: import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline 加载原始数据 In [4]: dataset = pd . read_csv ( './dataset/beijing.csv' ) dataset . head ( 5 ) Out[4]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } No year month day hour pm2.5 DEWP TEMP PRES cbwd Iws Is Ir 0 1 2010 1 1 0 NaN -21 -11.0 1021.0 NW 1.79 0 0 1 2 2010 1 1 1 NaN -21 -12.0 1020.0 NW 4.92 0 0 2 3 2010 1 1 2 NaN -21 -11.0 1019.0 NW 6.71 0 0 3 4 2010 1 1 3 NaN -21 -14.0 1019.0 NW 9.84 0 0 4 5 2010 1 1 4 NaN -20 -12.0 1018.0 NW 12.97 0 0 No: row number year: year of data in this row month: month of data in this row day: day of data in this row hour: hour of data in this row pm2.5: PM2.5 concentration (ug/m&#94;3) DEWP: Dew Point (â„ƒ) TEMP: Temperature (â„ƒ) PRES: Pressure (hPa) cbwd: Combined wind direction Iws: Cumulated wind speed (m/s) Is: Cumulated hours of snow Ir: Cumulated hours of rain 数据清洗 In [5]: dataset = pd . read_csv ( './dataset/beijing.csv' , header = 0 , parse_dates = [[ 1 , 2 , 3 , 4 ]], index_col = 0 , date_parser = lambda date : pd . datetime . strptime ( date , '%Y %m %d %H' )) dataset . drop ( 'No' , axis = 1 , inplace = True ) dataset . index . name = 'date' dataset . columns = [ 'pollution' , 'dew' , 'temp' , 'press' , 'wnd_dir' , 'wnd_spd' , 'snow' , 'rain' ] dataset . head () Out[5]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } pollution dew temp press wnd_dir wnd_spd snow rain date 2010-01-01 00:00:00 NaN -21 -11.0 1021.0 NW 1.79 0 0 2010-01-01 01:00:00 NaN -21 -12.0 1020.0 NW 4.92 0 0 2010-01-01 02:00:00 NaN -21 -11.0 1019.0 NW 6.71 0 0 2010-01-01 03:00:00 NaN -21 -14.0 1019.0 NW 9.84 0 0 2010-01-01 04:00:00 NaN -20 -12.0 1018.0 NW 12.97 0 0 In [6]: pd . isnull ( dataset ) . any () Out[6]: pollution True dew False temp False press False wnd_dir False wnd_spd False snow False rain False dtype: bool In [7]: #缺失数据用平均值填充 dataset = dataset . fillna ( dataset . mean ()[ 'pollution' ], axis = 1 ) In [8]: dataset . wnd_dir . unique () Out[8]: array(['NW', 'cv', 'NE', 'SE'], dtype=object) In [9]: dataset . wnd_dir = dataset . wnd_dir . map ({ 'NW' : 0 , 'cv' : 1 , 'NE' : 2 , 'SE' : 3 }) 保存数据清洗结果 In [10]: dataset . to_csv ( './dataset/beijing_pm25.csv' ) 加载数据 In [11]: data_set = pd . read_csv ( './dataset/beijing_pm25.csv' ) series = data_set . iloc [:, 1 ] series_values = series . values . astype ( 'float32' ) In [12]: plt . figure ( figsize = ( 20 , 12 )) for i in range ( 1 , data_set . shape [ 1 ]): plt . subplot ( data_set . shape [ 1 ], 1 , i + 1 ) plt . plot ( data_set . values [:, i ]) plt . title ( data_set . columns [ i ], y = 0.5 , loc = 'right' ) plt . show () 相关性分析 In [13]: corr_all = data_set . drop ( 'date' , axis = 1 ) . corr () mask = np . zeros_like ( corr_all , dtype = np . bool ) mask [ np . triu_indices_from ( mask )] = True f , ax = plt . subplots ( figsize = ( 10 , 6 )) sns . heatmap ( corr_all , mask = mask , linewidths = 0.25 , vmax = 1.0 , square = True , cmap = \"YlGnBu\" , linecolor = 'black' , annot = True ) plt . savefig ( './Correlation_Analysis.pdf' ) plt . show () 对于pm2.5来说没有看到比较强的相关指标 可以看到露水和温度呈现正相关特征 气压和露水呈现负相关特征 气压和温度呈现负相关特征 In [14]: from math import sqrt from numpy import concatenate from matplotlib import pyplot from pandas import read_csv from pandas import DataFrame from pandas import concat from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import LabelEncoder from sklearn.metrics import mean_squared_error from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM Using TensorFlow backend. In [15]: # convert series to supervised learning def series_to_supervised ( data , n_in = 1 , n_out = 1 , dropnan = True ): n_vars = 1 if type ( data ) is list else data . shape [ 1 ] df = DataFrame ( data ) cols , names = list (), list () # input sequence (t-n, ... t-1) for i in range ( n_in , 0 , - 1 ): cols . append ( df . shift ( i )) names += [( 'var %d (t- %d )' % ( j + 1 , i )) for j in range ( n_vars )] # forecast sequence (t, t+1, ... t+n) for i in range ( 0 , n_out ): cols . append ( df . shift ( - i )) if i == 0 : names += [( 'var %d (t)' % ( j + 1 )) for j in range ( n_vars )] else : names += [( 'var %d (t+ %d )' % ( j + 1 , i )) for j in range ( n_vars )] # put it all together agg = concat ( cols , axis = 1 ) agg . columns = names # drop rows with NaN values if dropnan : agg . dropna ( inplace = True ) return agg In [16]: # load dataset dataset = read_csv ( './dataset/beijing_pm25.csv' , header = 0 , index_col = 0 ) values = dataset . values # integer encode direction encoder = LabelEncoder () values [:, 4 ] = encoder . fit_transform ( values [:, 4 ]) # ensure all data is float values = values . astype ( 'float32' ) # normalize features scaler = MinMaxScaler ( feature_range = ( 0 , 1 )) scaled = scaler . fit_transform ( values ) # specify the number of lag hours n_hours = 3 n_features = 8 # frame as supervised learning reframed = series_to_supervised ( scaled , n_hours , 1 ) print ( reframed . shape ) (43821, 32) In [17]: # split into train and test sets values = reframed . values n_train_hours = 365 * 24 train = values [: n_train_hours , :] test = values [ n_train_hours :, :] # split into input and outputs n_obs = n_hours * n_features train_X , train_y = train [:, : n_obs ], train [:, - n_features ] test_X , test_y = test [:, : n_obs ], test [:, - n_features ] print ( train_X . shape , len ( train_X ), train_y . shape ) # reshape input to be 3D [samples, timesteps, features] train_X = train_X . reshape (( train_X . shape [ 0 ], n_hours , n_features )) test_X = test_X . reshape (( test_X . shape [ 0 ], n_hours , n_features )) print ( train_X . shape , train_y . shape , test_X . shape , test_y . shape ) (8760, 24) 8760 (8760,) (8760, 3, 8) (8760,) (35061, 3, 8) (35061,) In [18]: # design network model = Sequential () model . add ( LSTM ( 50 , input_shape = ( train_X . shape [ 1 ], train_X . shape [ 2 ]))) model . add ( Dense ( 1 )) model . compile ( loss = 'mae' , optimizer = 'adam' ) # fit network history = model . fit ( train_X , train_y , epochs = 50 , batch_size = 72 , validation_data = ( test_X , test_y ), verbose = 2 , shuffle = False ) # plot history pyplot . plot ( history . history [ 'loss' ], label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ], label = 'test' ) pyplot . legend () pyplot . show () Train on 8760 samples, validate on 35061 samples Epoch 1/50 2s - loss: 0.0533 - val_loss: 0.0423 Epoch 2/50 2s - loss: 0.0263 - val_loss: 0.0365 Epoch 3/50 3s - loss: 0.0205 - val_loss: 0.0222 Epoch 4/50 3s - loss: 0.0192 - val_loss: 0.0193 Epoch 5/50 3s - loss: 0.0188 - val_loss: 0.0190 Epoch 6/50 2s - loss: 0.0190 - val_loss: 0.0183 Epoch 7/50 2s - loss: 0.0188 - val_loss: 0.0179 Epoch 8/50 2s - loss: 0.0178 - val_loss: 0.0169 Epoch 9/50 2s - loss: 0.0175 - val_loss: 0.0173 Epoch 10/50 2s - loss: 0.0169 - val_loss: 0.0161 Epoch 11/50 2s - loss: 0.0166 - val_loss: 0.0164 Epoch 12/50 2s - loss: 0.0166 - val_loss: 0.0174 Epoch 13/50 3s - loss: 0.0161 - val_loss: 0.0157 Epoch 14/50 3s - loss: 0.0154 - val_loss: 0.0146 Epoch 15/50 2s - loss: 0.0149 - val_loss: 0.0142 Epoch 16/50 2s - loss: 0.0146 - val_loss: 0.0137 Epoch 17/50 2s - loss: 0.0144 - val_loss: 0.0135 Epoch 18/50 2s - loss: 0.0144 - val_loss: 0.0143 Epoch 19/50 2s - loss: 0.0142 - val_loss: 0.0136 Epoch 20/50 3s - loss: 0.0143 - val_loss: 0.0134 Epoch 21/50 2s - loss: 0.0139 - val_loss: 0.0135 Epoch 22/50 2s - loss: 0.0139 - val_loss: 0.0137 Epoch 23/50 2s - loss: 0.0140 - val_loss: 0.0134 Epoch 24/50 2s - loss: 0.0138 - val_loss: 0.0137 Epoch 25/50 2s - loss: 0.0142 - val_loss: 0.0138 Epoch 26/50 2s - loss: 0.0138 - val_loss: 0.0142 Epoch 27/50 2s - loss: 0.0142 - val_loss: 0.0134 Epoch 28/50 2s - loss: 0.0139 - val_loss: 0.0137 Epoch 29/50 2s - loss: 0.0138 - val_loss: 0.0136 Epoch 30/50 2s - loss: 0.0141 - val_loss: 0.0136 Epoch 31/50 2s - loss: 0.0138 - val_loss: 0.0137 Epoch 32/50 2s - loss: 0.0141 - val_loss: 0.0134 Epoch 33/50 2s - loss: 0.0137 - val_loss: 0.0136 Epoch 34/50 2s - loss: 0.0139 - val_loss: 0.0137 Epoch 35/50 2s - loss: 0.0138 - val_loss: 0.0138 Epoch 36/50 2s - loss: 0.0136 - val_loss: 0.0136 Epoch 37/50 2s - loss: 0.0142 - val_loss: 0.0138 Epoch 38/50 2s - loss: 0.0138 - val_loss: 0.0136 Epoch 39/50 2s - loss: 0.0140 - val_loss: 0.0137 Epoch 40/50 2s - loss: 0.0139 - val_loss: 0.0136 Epoch 41/50 2s - loss: 0.0141 - val_loss: 0.0136 Epoch 42/50 2s - loss: 0.0141 - val_loss: 0.0138 Epoch 43/50 2s - loss: 0.0141 - val_loss: 0.0144 Epoch 44/50 2s - loss: 0.0140 - val_loss: 0.0142 Epoch 45/50 2s - loss: 0.0140 - val_loss: 0.0143 Epoch 46/50 2s - loss: 0.0139 - val_loss: 0.0141 Epoch 47/50 2s - loss: 0.0138 - val_loss: 0.0143 Epoch 48/50 2s - loss: 0.0139 - val_loss: 0.0143 Epoch 49/50 2s - loss: 0.0139 - val_loss: 0.0145 Epoch 50/50 2s - loss: 0.0139 - val_loss: 0.0143 In [19]: # make a prediction yhat = model . predict ( test_X ) test_X = test_X . reshape (( test_X . shape [ 0 ], n_hours * n_features )) # invert scaling for forecast inv_yhat = concatenate (( yhat , test_X [:, - 7 :]), axis = 1 ) inv_yhat = scaler . inverse_transform ( inv_yhat ) inv_yhat = inv_yhat [:, 0 ] # invert scaling for actual test_y = test_y . reshape (( len ( test_y ), 1 )) inv_y = concatenate (( test_y , test_X [:, - 7 :]), axis = 1 ) inv_y = scaler . inverse_transform ( inv_y ) inv_y = inv_y [:, 0 ] # calculate RMSE rmse = sqrt ( mean_squared_error ( inv_y , inv_yhat )) print ( 'Test RMSE: %.3f ' % rmse ) Test RMSE: 24.990 In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Correlation Analysis","url":"Correlation_Analysis.html"},{"title":"PM2.5 Correlation Analysis","text":"北京PM2.5相关性分析 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline 加载原始数据 dataset = pd . read_csv ( './dataset/beijing.csv' ) dataset . head ( 5 ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } No year month day hour pm2.5 DEWP TEMP PRES cbwd Iws Is Ir 0 1 2010 1 1 0 NaN -21 -11.0 1021.0 NW 1.79 0 0 1 2 2010 1 1 1 NaN -21 -12.0 1020.0 NW 4.92 0 0 2 3 2010 1 1 2 NaN -21 -11.0 1019.0 NW 6.71 0 0 3 4 2010 1 1 3 NaN -21 -14.0 1019.0 NW 9.84 0 0 4 5 2010 1 1 4 NaN -20 -12.0 1018.0 NW 12.97 0 0 No: row number year: year of data in this row month: month of data in this row day: day of data in this row hour: hour of data in this row pm2.5: PM2.5 concentration (ug/m&#94;3) DEWP: Dew Point (â„ƒ) TEMP: Temperature (â„ƒ) PRES: Pressure (hPa) cbwd: Combined wind direction Iws: Cumulated wind speed (m/s) Is: Cumulated hours of snow Ir: Cumulated hours of rain 数据清洗 dataset = pd . read_csv ( './dataset/beijing.csv' , header = 0 , parse_dates = [[ 1 , 2 , 3 , 4 ]], index_col = 0 , date_parser = lambda date : pd . datetime . strptime ( date , '%Y %m %d %H' )) dataset . drop ( 'No' , axis = 1 , inplace = True ) dataset . index . name = 'date' dataset . columns = [ 'pollution' , 'dew' , 'temp' , 'press' , 'wnd_dir' , 'wnd_spd' , 'snow' , 'rain' ] dataset . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } pollution dew temp press wnd_dir wnd_spd snow rain date 2010-01-01 00:00:00 NaN -21 -11.0 1021.0 NW 1.79 0 0 2010-01-01 01:00:00 NaN -21 -12.0 1020.0 NW 4.92 0 0 2010-01-01 02:00:00 NaN -21 -11.0 1019.0 NW 6.71 0 0 2010-01-01 03:00:00 NaN -21 -14.0 1019.0 NW 9.84 0 0 2010-01-01 04:00:00 NaN -20 -12.0 1018.0 NW 12.97 0 0 pd . isnull ( dataset ) . any () pollution True dew False temp False press False wnd_dir False wnd_spd False snow False rain False dtype: bool #缺失数据用平均值填充 dataset = dataset . fillna ( dataset . mean ()[ 'pollution' ], axis = 1 ) dataset . wnd_dir . unique () array(['NW', 'cv', 'NE', 'SE'], dtype=object) dataset . wnd_dir = dataset . wnd_dir . map ({ 'NW' : 0 , 'cv' : 1 , 'NE' : 2 , 'SE' : 3 }) 保存数据清洗结果 dataset . to_csv ( './dataset/beijing_pm25.csv' ) 加载数据 data_set = pd . read_csv ( './dataset/beijing_pm25.csv' ) series = data_set . iloc [:, 1 ] series_values = series . values . astype ( 'float32' ) plt . figure ( figsize = ( 20 , 12 )) for i in range ( 1 , data_set . shape [ 1 ]): plt . subplot ( data_set . shape [ 1 ], 1 , i + 1 ) plt . plot ( data_set . values [:, i ]) plt . title ( data_set . columns [ i ], y = 0.5 , loc = 'right' ) plt . show () 相关性分析 corr_all = data_set . drop ( 'date' , axis = 1 ) . corr () mask = np . zeros_like ( corr_all , dtype = np . bool ) mask [ np . triu_indices_from ( mask )] = True f , ax = plt . subplots ( figsize = ( 10 , 6 )) sns . heatmap ( corr_all , mask = mask , linewidths = 0.25 , vmax = 1.0 , square = True , cmap = \"YlGnBu\" , linecolor = 'black' , annot = True ) plt . savefig ( './Correlation_Analysis.pdf' ) plt . show () 对于pm2.5来说没有看到比较强的相关指标 可以看到露水和温度呈现正相关特征 气压和露水呈现负相关特征 气压和温度呈现负相关特征","tags":"Correlation Analysis","url":"pm25-correlation-analysis.html"},{"title":"My First Review","text":"Following is a review of my favorite mechanical keyboard.","tags":"Review","url":"my-first-review.html"}]}